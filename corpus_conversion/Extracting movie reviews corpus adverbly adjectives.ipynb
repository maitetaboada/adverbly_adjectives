{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os, re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [8.0, 6.0]\n",
    "colours = itertools.cycle(sns.color_palette('dark'))\n",
    "palettes = itertools.cycle([sns.color_palette('PuBuGn_d'), sns.color_palette('Oranges_d'), sns.color_palette('GnBu_d'), sns.color_palette('Reds_d'), sns.color_palette('Blues_d'), sns.color_palette('PuRd_d')])\n",
    "import csv\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en')\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './movies-data-v1.0/reviews/'\n",
    "files = [ path+d+'/'+f for d in os.listdir(path) if os.path.isdir(path+d) for f in os.listdir(path+d) ]\n",
    "core_texts = []\n",
    "\n",
    "def deHTML(comment):\n",
    "    bs = BeautifulSoup(comment, 'lxml').text\n",
    "    newlinetabs_removed = re.sub('[\\n\\t\\r]', ' ', bs)\n",
    "    extraws_removed = re.sub('\\s\\s+', ' ', newlinetabs_removed)\n",
    "    res = re.sub('�', '\\'', extraws_removed)\n",
    "    res = re.sub('', '', res)\n",
    "    return res.strip()\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r') as fo:\n",
    "        fo.readline()\n",
    "        text = fo.read()\n",
    "        core_texts.append(deHTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_sets = []\n",
    "for text in core_texts:\n",
    "    core_sets.append(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n"
     ]
    }
   ],
   "source": [
    "sim_dict = defaultdict(int)\n",
    "similarities = []\n",
    "for index,set1 in enumerate(core_sets):\n",
    "    if index % 50 == 0: print(index)\n",
    "    for index1,set2 in enumerate(core_sets):\n",
    "        if (index != index1) and (sim_dict[(index,index1)] == 0):\n",
    "            sim_score = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "            similarities.append([core_texts[index], core_texts[index1], sim_score])\n",
    "            \n",
    "            sim_dict[(index,index1)] = 1\n",
    "            sim_dict[(index1,index)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24659615</th>\n",
       "      <td>Apr 17, 2008 What would it take to draw people...</td>\n",
       "      <td>Apr 17, 2008 What would it take to draw people...</td>\n",
       "      <td>0.994236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24871293</th>\n",
       "      <td>Dec 03, 2008 Frost/Nixon is a fact-based drama...</td>\n",
       "      <td>Dec 03, 2008 Frost/Nixon is a fact-based drama...</td>\n",
       "      <td>0.995316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24883176</th>\n",
       "      <td>Mar 05, 2008 Married Life takes place in an im...</td>\n",
       "      <td>Mar 05, 2008 Miss Pettigrew Lives for a Day is...</td>\n",
       "      <td>0.995565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0  \\\n",
       "24659615  Apr 17, 2008 What would it take to draw people...   \n",
       "24871293  Dec 03, 2008 Frost/Nixon is a fact-based drama...   \n",
       "24883176  Mar 05, 2008 Married Life takes place in an im...   \n",
       "\n",
       "                                                          1         2  \n",
       "24659615  Apr 17, 2008 What would it take to draw people...  0.994236  \n",
       "24871293  Dec 03, 2008 Frost/Nixon is a fact-based drama...  0.995316  \n",
       "24883176  Mar 05, 2008 Miss Pettigrew Lives for a Day is...  0.995565  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df[(sim_df[2] > 0.5) & (sim_df[2] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for delete in list(sim_df[(sim_df[2] > 0.5) & (sim_df[2] < 1)][1].values):\n",
    "    try:\n",
    "        del core_texts[core_texts.index(delete)]\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7012"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(list(set(core_texts))).index)\n",
    "# should be 7012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_texts = list(set(core_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movies_all_reviews.txt', 'w') as fo:\n",
    "    for text in core_texts:\n",
    "        fo.writelines(text+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_list = [i[0] for i in pd.DataFrame(core_texts).values]\n",
    "# genre_list = [i[0] for i in pd.DataFrame(core_texts).values]\n",
    "        \n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "freq = 0\n",
    "adj_count = 0\n",
    "\n",
    "for index,doc in enumerate(nlp.pipe(flat_list, batch_size=50, n_threads=80)):\n",
    "    print('still going:', index)\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # to make sure the indices will line up\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'blahblahblha'\n",
    "word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'blahbl_ahblha'\n",
    "word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_adj_dict = defaultdict(int)\n",
    "wordcount_dict = defaultdict(int)\n",
    "adv_adj_list = []\n",
    "\n",
    "for comment_index, comment in enumerate(pos):\n",
    "    length = len(comment)\n",
    "    genre = 'blah'\n",
    "    if genre is not None:\n",
    "        for word_index, part in enumerate(comment):\n",
    "            \n",
    "            if (part == 'ADJ'):\n",
    "                adj_count += 1\n",
    "\n",
    "            # for each adverb ending in -ly\n",
    "            if (part == 'ADV') and tokens[comment_index][word_index][-2:] == 'ly':\n",
    "                # if the next word is adj\n",
    "                if (word_index+1 < length):\n",
    "                    if (comment[word_index+1] == 'ADJ'):\n",
    "                        word1 = tokens[comment_index][word_index].lower()\n",
    "                        word2 = tokens[comment_index][word_index+1].lower()\n",
    "                        \n",
    "                        if word1 != 'only':                        \n",
    "                            if (word1.isalpha() and word2.isalpha()):\n",
    "                                # count it\n",
    "                                freq = freq + 1\n",
    "                                # dict key is 'word1 word2' in lowercase\n",
    "                                key = tokens[comment_index][word_index].lower() + \" \" + tokens[comment_index][word_index+1].lower()\n",
    "                                # add to the dict\n",
    "                                adv_adj_dict[key] += 1\n",
    "                            else:\n",
    "                                print(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "for comment_index,text in enumerate(pos):\n",
    "    for word_index,part in enumerate(text):\n",
    "        pos_list.append([comment_index, tokens[comment_index][word_index], part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pos_list).rename(columns={0 : 'text_num', 1 : 'token', 2 : 'pos'}).to_csv('Movies_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Freq: \" +str(freq))\n",
    "print(\"Adjective count: \" +str(adj_count))\n",
    "\n",
    "print('Number of tokens')\n",
    "print(sum([len(sublist) for sublist in pos]))\n",
    "\n",
    "types = set([i for sublist in tokens for i in sublist])\n",
    "print('Overall lexical diversity')\n",
    "print(len(types) / len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('adj_genre_dict.csv', 'w+') as csv_file:\n",
    "#     writer = csv.writer(csv_file)\n",
    "#     for key, value in adj_genre_dict.items():\n",
    "#        writer.writerow([key, value])\n",
    "\n",
    "# with open('advadj_genre_dict.csv', 'w+') as csv_file:\n",
    "#     writer = csv.writer(csv_file)\n",
    "#     for key, value in adv_adj_genre_dict.items():\n",
    "#        writer.writerow([key, value])\n",
    "\n",
    "with open('advadj_dict.csv', 'w+') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in adv_adj_dict.items():\n",
    "       writer.writerow([key, value])\n",
    "    \n",
    "# with open('genre_wordcounts.csv', 'w+') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     for key, value in wordcount_dict.items():\n",
    "#        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"overview.txt\", \"w+\") as f:\n",
    "    f.write(\"Frequency: \" + str(freq) + \"\\n\")\n",
    "    f.write(\"Adjective count: \" + str(adj_count) + \"\\n\")\n",
    "    f.write(\"Number of tokens: \" + str(sum([len(sublist) for sublist in pos])) + \"\\n\")\n",
    "    types = set([i for sublist in tokens for i in sublist])\n",
    "    f.write(\"Overall lexical diversity: \" + str(len(types) / len(tokens)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(sublist) for sublist in pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
