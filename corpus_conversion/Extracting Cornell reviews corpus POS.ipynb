{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os, re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [8.0, 6.0]\n",
    "colours = itertools.cycle(sns.color_palette('dark'))\n",
    "palettes = itertools.cycle([sns.color_palette('PuBuGn_d'), sns.color_palette('Oranges_d'), sns.color_palette('GnBu_d'), sns.color_palette('Reds_d'), sns.color_palette('Blues_d'), sns.color_palette('PuRd_d')])\n",
    "import csv\n",
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en')\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Cornell_reviews/raw/'\n",
    "files = [ path+d+'/'+f for d in os.listdir(path) if os.path.isdir(path+d) for f in os.listdir(path+d) ]\n",
    "core_texts = []\n",
    "\n",
    "def deHTML(comment):\n",
    "    bs = BeautifulSoup(comment, 'lxml').text\n",
    "    newlinetabs_removed = re.sub('[\\n\\t\\r]', ' ', bs)\n",
    "    extraws_removed = re.sub('\\s\\s+', ' ', newlinetabs_removed)\n",
    "    res = re.sub('�', '\\'', extraws_removed)\n",
    "    res = re.sub('', '', res)\n",
    "    return res.strip()\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r') as fo:\n",
    "        fo.readline()\n",
    "        text = fo.read()\n",
    "        core_texts.append(deHTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_sets = []\n",
    "for text in core_texts:\n",
    "    core_sets.append(set(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "sim_dict = defaultdict(int)\n",
    "similarities = []\n",
    "for index,set1 in enumerate(core_sets):\n",
    "    if index % 50 == 0: print(index)\n",
    "    for index1,set2 in enumerate(core_sets):\n",
    "        if (index != index1) and (sim_dict[(index,index1)] == 0):\n",
    "            sim_score = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "            similarities.append([core_texts[index], core_texts[index1], sim_score])\n",
    "            \n",
    "            sim_dict[(index,index1)] = 1\n",
    "            sim_dict[(index1,index)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.DataFrame(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53994</th>\n",
       "      <td>first , let me assure you that no prior experi...</td>\n",
       "      <td>first , let me assure you that no prior experi...</td>\n",
       "      <td>0.997881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277935</th>\n",
       "      <td>i still think that \" the bridge on the river k...</td>\n",
       "      <td>i still think that it doesn't deserve being th...</td>\n",
       "      <td>0.929078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509118</th>\n",
       "      <td>\" fun \" is a noun , and therefore cannot be co...</td>\n",
       "      <td>\" fun \" is a noun , and therefore cannot be co...</td>\n",
       "      <td>0.996546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050339</th>\n",
       "      <td>event horizon is not your run-of-the-mill sci-...</td>\n",
       "      <td>event = horizon is not your run-of-the-mill sc...</td>\n",
       "      <td>0.982993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652451</th>\n",
       "      <td>ben kingsley ? \" my response would be that any...</td>\n",
       "      <td>his old associates want him for one last job a...</td>\n",
       "      <td>0.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690618</th>\n",
       "      <td>what better for the farrelly brothers , famous...</td>\n",
       "      <td>what better for the farrelly brothers , famous...</td>\n",
       "      <td>0.998296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869343</th>\n",
       "      <td>they seem like dangerous rocks because they ki...</td>\n",
       "      <td>same old attitude . don't forget to recommend ...</td>\n",
       "      <td>0.929134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978604</th>\n",
       "      <td>so i didn't expect too much from this . but i ...</td>\n",
       "      <td>so i didn't expect too much from this . but i ...</td>\n",
       "      <td>0.997403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0  \\\n",
       "53994    first , let me assure you that no prior experi...   \n",
       "277935   i still think that \" the bridge on the river k...   \n",
       "509118   \" fun \" is a noun , and therefore cannot be co...   \n",
       "1050339  event horizon is not your run-of-the-mill sci-...   \n",
       "1652451  ben kingsley ? \" my response would be that any...   \n",
       "1690618  what better for the farrelly brothers , famous...   \n",
       "1869343  they seem like dangerous rocks because they ki...   \n",
       "1978604  so i didn't expect too much from this . but i ...   \n",
       "\n",
       "                                                         1         2  \n",
       "53994    first , let me assure you that no prior experi...  0.997881  \n",
       "277935   i still think that it doesn't deserve being th...  0.929078  \n",
       "509118   \" fun \" is a noun , and therefore cannot be co...  0.996546  \n",
       "1050339  event = horizon is not your run-of-the-mill sc...  0.982993  \n",
       "1652451  his old associates want him for one last job a...  0.942500  \n",
       "1690618  what better for the farrelly brothers , famous...  0.998296  \n",
       "1869343  same old attitude . don't forget to recommend ...  0.929134  \n",
       "1978604  so i didn't expect too much from this . but i ...  0.997403  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df[(sim_df[2] > 0.5) & (sim_df[2] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for delete in list(sim_df[(sim_df[2] > 0.5) & (sim_df[2] < 1)][1].values):\n",
    "    try:\n",
    "        del core_texts[core_texts.index(delete)]\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1992"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(list(set(core_texts))).index)\n",
    "# should be 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_texts = list(set(core_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cornell_all_reviews.txt', 'w') as fo:\n",
    "    for text in core_texts:\n",
    "        fo.writelines(text+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_list = [i[0] for i in pd.DataFrame(core_texts).values]\n",
    "# genre_list = [i[0] for i in pd.DataFrame(core_texts).values]\n",
    "        \n",
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "freq = 0\n",
    "adj_count = 0\n",
    "\n",
    "for index,doc in enumerate(nlp.pipe(flat_list, batch_size=50, n_threads=80)):\n",
    "    print('still going:', index)\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # to make sure the indices will line up\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_adj_dict = defaultdict(int)\n",
    "wordcount_dict = defaultdict(int)\n",
    "adv_adj_list = []\n",
    "\n",
    "for comment_index, comment in enumerate(pos):\n",
    "    length = len(comment)\n",
    "    genre = 'blah'\n",
    "    if genre is not None:\n",
    "        for word_index, part in enumerate(comment):\n",
    "            \n",
    "            if (part == 'ADJ'):\n",
    "                adj_count += 1\n",
    "\n",
    "            # for each adverb ending in -ly\n",
    "            if (part == 'ADV') and tokens[comment_index][word_index][-2:] == 'ly':\n",
    "                # if the next word is adj\n",
    "                if (word_index+1 < length):\n",
    "                    if (comment[word_index+1] == 'ADJ'):\n",
    "                        word1 = tokens[comment_index][word_index].lower()\n",
    "                        word2 = tokens[comment_index][word_index+1].lower()\n",
    "                        \n",
    "                        if word1 != 'only':                        \n",
    "                            if (word1.isalpha() and word2.isalpha()):\n",
    "                                # count it\n",
    "                                freq = freq + 1\n",
    "                                # dict key is 'word1 word2' in lowercase\n",
    "                                key = tokens[comment_index][word_index].lower() + \" \" + tokens[comment_index][word_index+1].lower()\n",
    "                                # add to the dict\n",
    "                                adv_adj_dict[key] += 1\n",
    "                            else:\n",
    "                                print(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "for comment_index,text in enumerate(pos):\n",
    "    for word_index,part in enumerate(text):\n",
    "        pos_list.append([comment_index, tokens[comment_index][word_index], part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pos_list).rename(columns={0 : 'text_num', 1 : 'token', 2 : 'pos'}).to_csv('Cornell_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Freq: \" +str(freq))\n",
    "print(\"Adjective count: \" +str(adj_count))\n",
    "\n",
    "print('Number of tokens')\n",
    "print(sum([len(sublist) for sublist in pos]))\n",
    "\n",
    "types = set([i for sublist in tokens for i in sublist])\n",
    "print('Overall lexical diversity')\n",
    "print(len(types) / len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('adj_genre_dict.csv', 'w+') as csv_file:\n",
    "#     writer = csv.writer(csv_file)\n",
    "#     for key, value in adj_genre_dict.items():\n",
    "#        writer.writerow([key, value])\n",
    "\n",
    "# with open('advadj_genre_dict.csv', 'w+') as csv_file:\n",
    "#     writer = csv.writer(csv_file)\n",
    "#     for key, value in adv_adj_genre_dict.items():\n",
    "#        writer.writerow([key, value])\n",
    "\n",
    "with open('advadj_dict.csv', 'w+') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in adv_adj_dict.items():\n",
    "       writer.writerow([key, value])\n",
    "    \n",
    "# with open('genre_wordcounts.csv', 'w+') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     for key, value in wordcount_dict.items():\n",
    "#        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"overview.txt\", \"w+\") as f:\n",
    "    f.write(\"Frequency: \" + str(freq) + \"\\n\")\n",
    "    f.write(\"Adjective count: \" + str(adj_count) + \"\\n\")\n",
    "    f.write(\"Number of tokens: \" + str(sum([len(sublist) for sublist in pos])) + \"\\n\")\n",
    "    types = set([i for sublist in tokens for i in sublist])\n",
    "    f.write(\"Overall lexical diversity: \" + str(len(types) / len(tokens)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(sublist) for sublist in pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
